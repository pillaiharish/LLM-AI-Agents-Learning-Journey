id,model_name,url,type,params_billion,typical_use,est_vram_4bit_gb,approx_vram_gb,vram_tier,kv_cache_support,sharding,quant_formats,determinism_tip,good_to_know,notes,context_window_tokens,max_output_tokens,tokens_per_second_note,input_modalities,image_formats_example,prompt_template,tokenizer,framework_support,quantization_available,recommended_batch_size_4bit,latency_note,throughput_note,io_note,compat_note,good_to_know_2
1,Llama-3.1-8B-Instruct,https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct,LLM,8.0,General chat / coding,6–7,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,128K ctx family; popular 8B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Llama-3 Instruct (meta),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
2,Mistral-7B-v0.3,https://huggingface.co/mistralai/Mistral-7B-v0.3,LLM,7.3,General chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Efficient 7B; many finetunes.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
3,Qwen2.5-7B-Instruct,https://huggingface.co/Qwen/Qwen2.5-7B-Instruct,LLM,7.0,General chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Long-context variants exist.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
4,gemma-2-9b-it,https://huggingface.co/google/gemma-2-9b-it,LLM,9.0,General chat / reasoning,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Good small generalist.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
5,stablelm-2-12b-chat,https://huggingface.co/stabilityai/stablelm-2-12b-chat,LLM,12.1,General chat,8–10,8.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Multilingual small model.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
6,Mixtral-8x7B-Instruct-v0.1,https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1,LLM (MoE),45.0,General / RAG,24–28,24.0,24–<32 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,MoE (~12B active); 4-bit fits 24–28 GB.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
7,starcoder2-15b,https://huggingface.co/bigcode/starcoder2-15b,Code LLM,15.0,Code generation,10–12,10.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,16K ctx; permissive.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
8,DeepSeek-Coder-V2-Instruct,https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct,Code LLM (MoE),16.0,Coding / fill-in-the-middle,11–13,11.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,MoE family; strong coding.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
9,CodeLlama-13b-Instruct-hf,https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf,Code LLM,13.0,Coding / infill,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Solid OSS code baseline.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
10,Llama-3.2-3B-Instruct,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,LLM,3.0,Lightweight chat,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Good for small GPUs.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Llama-3 Instruct (meta),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
11,Llama-3.2-1B-Instruct,https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct,SLM,1.0,Ultra-light chat,1–2,1.0,1–<2 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Tiny model: great for edge; consider int4 + CPU fallback if VRAM tight.,Edge devices.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Llama-3 Instruct (meta),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
12,OpenELM-3B,https://huggingface.co/apple/OpenELM-3B,SLM,3.0,Lightweight chat,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Apple small model.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
13,btlm-3b-8k,https://huggingface.co/cerebras/btlm-3b-8k,SLM,3.0,Lightweight chat,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,BTLM 3B 8k.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
14,TinyLlama-1.1B-Chat-v1.0,https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0,SLM,1.1,Tiny chat,1–2,1.0,1–<2 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Tiny model: great for edge; consider int4 + CPU fallback if VRAM tight.,Great for very small GPUs.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
15,idefics2-8b,https://huggingface.co/HuggingFaceM4/idefics2-8b,VLM,8.0,Vision-language chat / OCR,7–9,7.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Apache-2.0 VLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),VLM: vision + text (repo-specific format),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
16,llava-v1.6-mistral-7b,https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b,VLM,7.0,Vision-chat / caption,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Popular 7B VLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
17,Pixtral-12B,https://huggingface.co/mistralai/Pixtral-12B,VLM,12.0,Doc/image QA,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,128K ctx VLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),VLM: vision + text (repo-specific format),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
18,InternVL2-8B,https://huggingface.co/OpenGVLab/InternVL2-8B,VLM,8.0,"Doc/chart OCR, VQA",6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Strong doc understanding.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),VLM: vision + text (repo-specific format),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
19,Qwen2.5-VL-7B-Instruct,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,VLM,7.0,Vision-chat / video,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Vision variant of Qwen2.5.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
20,Llama-3.2-11B-Vision-Instruct,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,VLM,11.0,Vision + text chat,8–10,8.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Tiny model: great for edge; consider int4 + CPU fallback if VRAM tight.,Meta vision model.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Llama-3 Instruct (meta),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
21,Qwen2.5-Coder-32B-Instruct,https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct,Code LLM,32.0,Advanced coding,18–22,18.0,18–<24 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Large code model.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
22,codegemma-7b-it,https://huggingface.co/google/codegemma-7b-it,Code LLM,7.0,Coding,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Google CodeGemma.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
23,starcoder2-3b,https://huggingface.co/bigcode/starcoder2-3b,Code LLM,3.0,Coding,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Small StarCoder2.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
24,Codestral-22B-v0.1,https://huggingface.co/mistralai/Codestral-22B-v0.1,Code LLM,22.0,Coding / reasoning,14–16,14.0,12–<16 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Strong 22B code LLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
25,glm-4-9b-chat-hf,https://huggingface.co/zai-org/glm-4-9b-chat-hf,LLM,9.0,Chat,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,GLM-4 9B chat.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
26,codegeex4-all-9b,https://huggingface.co/THUDM/codegeex4-all-9b,Code LLM,9.0,Coding,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,New CodeGeeX4 9B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
27,Mistral-NeMo-12B-Instruct,https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct,LLM,12.0,Chat / coding,8–10,8.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,128K ctx.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
28,AI21-Jamba-Mini-1.7,https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7,LLM (MoE),1.7,Chat / tools,2–3,2.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,Jamba Mini.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
29,MAmmoTH2-8x7B,https://huggingface.co/TIGER-Lab/MAmmoTH2-8x7B,LLM (MoE),45.0,Reasoning / RAG,24–28,24.0,24–<32 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,MoE 8×7B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
30,openchat-3.5-1210,https://huggingface.co/openchat/openchat-3.5-1210,LLM (fine-tune),7.0,Helpful chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Community SFT/DPO.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
31,Nous-Hermes-2-Mistral-7B-DPO,https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO,LLM (fine-tune),7.0,Assistant chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Popular Mistral SFT.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
32,OpenHermes-2.5-Mistral-7B,https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B,LLM (fine-tune),7.0,Assistant chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Community favorite.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
33,replit-code-v1_5-3b,https://huggingface.co/replit/replit-code-v1_5-3b,Code LLM,3.0,Coding,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Replit small code.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
34,stable-code-3b,https://huggingface.co/Stability-AI/stable-code-3b,Code LLM,3.0,Coding,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,StableCode 3B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
35,Qwen2.5-72B-Instruct,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,LLM,72.0,Advanced chat,38–45,38.0,32–<64 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Large: prefer multi-GPU or server-class VRAM; enable paged attention & flash-attn if supported.,Multi-GPU recommended.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
36,granite-3.3-8b-instruct,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,LLM,8.0,Chat,6–7,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,IBM Granite 8B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
37,granite-20b-code-instruct-8k,https://huggingface.co/ibm-granite/granite-20b-code-instruct-8k,Code LLM,20.0,Coding,12–14,12.0,12–<16 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Granite Code 20B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
38,Phi-3-medium-128k-instruct,https://huggingface.co/microsoft/Phi-3-medium-128k-instruct,LLM,14.0,Chat / reasoning,10–12,10.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Long-context.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Phi-style chat (see card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
39,Phi-3.5-vision-instruct,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,VLM,,Vision + text,6–9,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Params N/A; compact VLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Phi-style chat (see card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
40,MiniCPM-V-2_6,https://huggingface.co/openbmb/MiniCPM-V-2_6,VLM,8.0,Vision QA / OCR,7–9,7.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Tiny model: great for edge; consider int4 + CPU fallback if VRAM tight.,Lightweight VLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
41,Qwen2.5-Coder-32B-Instruct,https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct,Code LLM,32.0,Advanced coding,18–22,18.0,18–<24 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #21.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
42,codegemma-7b-it,https://huggingface.co/google/codegemma-7b-it,Code LLM,7.0,Coding,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #22.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
43,starcoder2-3b,https://huggingface.co/bigcode/starcoder2-3b,Code LLM,3.0,Coding,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #23.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
44,Codestral-22B-v0.1,https://huggingface.co/mistralai/Codestral-22B-v0.1,Code LLM,22.0,Coding / reasoning,14–16,14.0,12–<16 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #24.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
45,glm-4-9b-chat-hf,https://huggingface.co/zai-org/glm-4-9b-chat-hf,LLM,9.0,Chat,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #25.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
46,codegeex4-all-9b,https://huggingface.co/THUDM/codegeex4-all-9b,Code LLM,9.0,Coding,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #26.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
47,Mistral-NeMo-12B-Instruct,https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct,LLM,12.0,Chat / coding,8–10,8.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #27.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
48,AI21-Jamba-Mini-1.7,https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7,LLM (MoE),1.7,Chat / tools,2–3,2.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,Duplicate of #28.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
49,DeepSeek-Coder-V2-Instruct,https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct,Code LLM (MoE),16.0,Coding / FIM,11–13,11.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,Duplicate of #8.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
50,DeepSeek-V2,https://huggingface.co/deepseek-ai/DeepSeek-V2,LLM (MoE),,Advanced chat / reasoning,>30,30.0,24–<32 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,Very large MoE; multi-GPU.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
51,MAmmoTH2-8x7B,https://huggingface.co/TIGER-Lab/MAmmoTH2-8x7B,LLM (MoE),45.0,Reasoning / RAG,24–28,24.0,24–<32 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,Duplicate of #29.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
52,openchat-3.5-1210,https://huggingface.co/openchat/openchat-3.5-1210,LLM (fine-tune),7.0,Helpful chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #30.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
53,Nous-Hermes-2-Mistral-7B-DPO,https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO,LLM (fine-tune),7.0,Assistant chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #31.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
54,OpenHermes-2.5-Mistral-7B,https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B,LLM (fine-tune),7.0,Assistant chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #32.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
55,replit-code-v1_5-3b,https://huggingface.co/replit/replit-code-v1_5-3b,Code LLM,3.0,Coding,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #33.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
56,stable-code-3b,https://huggingface.co/Stability-AI/stable-code-3b,Code LLM,3.0,Coding,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #34.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
57,Qwen2.5-72B-Instruct,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,LLM,72.0,Advanced chat,38–45,38.0,32–<64 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Large: prefer multi-GPU or server-class VRAM; enable paged attention & flash-attn if supported.,Duplicate of #35.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
58,granite-3.3-8b-instruct,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,LLM,8.0,Chat,6–7,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #36.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
59,granite-20b-code-instruct-8k,https://huggingface.co/ibm-granite/granite-20b-code-instruct-8k,Code LLM,20.0,Coding,12–14,12.0,12–<16 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #37.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
60,Phi-3-medium-128k-instruct,https://huggingface.co/microsoft/Phi-3-medium-128k-instruct,LLM,14.0,Chat / reasoning,10–12,10.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Duplicate of #38.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Phi-style chat (see card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
61,llava-next (docs),https://huggingface.co/docs/transformers/en/model_doc/llava_next,VLM,,Vision-language family,6–12,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Family docs; pick 7B variants to run.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),VLM: vision + text (repo-specific format),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
62,llava-onevision-qwen2-7b-ov,https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov,VLM,7.0,OneVision VLM,6–8,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Qwen2-7B base.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
63,moondream2,https://huggingface.co/vikhyatk/moondream2,VLM (tiny),1.8,Mobile-scale VQA,2–3,2.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Very small VLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
64,Florence-2-large,https://huggingface.co/microsoft/Florence-2-large,Vision encoder,,OCR/grounding,>10,10.0,8–<12 GB,N/A (encoder),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Often used with LLM head.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
65,DocOwl1.5,https://huggingface.co/mPLUG/DocOwl1.5,VLM,,Doc understanding,6–10,6.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,PDF/table focus.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
66,VILA-13b,https://huggingface.co/Efficient-Large-Model/VILA-13b,VLM,13.0,Vision-language,10–12,10.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,13B VLM.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
67,llava-v1.6-34b-hf,https://huggingface.co/llava-hf/llava-v1.6-34b-hf,VLM,34.0,High-end VLM,18–22,18.0,18–<24 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Large 34B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),VLM: vision + text (repo-specific format),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
68,Qwen2-VL-2B-Instruct,https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct,VLM,2.0,Lightweight VLM,2–3,2.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Small vision model.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),ChatML (Qwen),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
69,InternVL2-26B,https://huggingface.co/OpenGVLab/InternVL2-26B,VLM,26.0,Doc/chart QA,14–18,14.0,12–<16 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Larger InternVL2.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),VLM: vision + text (repo-specific format),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
70,Yi-VL-6B,https://huggingface.co/01-ai/Yi-VL-6B,VLM,6.0,Vision-chat,5–7,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp VLM variants), AWQ/GPTQ/bnb-4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",VLM: VRAM scales with image resolution & #images; enable image preproc and smaller vision tower if offered.,Yi VLM 6B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text + Image,JPEG/PNG; others per repo (WEBP/GIF/TIFF may vary),Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Resize/normalize images per repo; watch max image pixels / sequence length,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
71,OLMo-2-1124-7B-Instruct,https://huggingface.co/allenai/OLMo-2-1124-7B-Instruct,LLM,7.0,General chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,AllenAI OLMo-2.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
72,dbrx-instruct,https://huggingface.co/databricks/dbrx-instruct,LLM (MoE),,Advanced chat,>30,30.0,24–<32 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,Very large MoE (multi-GPU).,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
73,mpt-7b-instruct,https://huggingface.co/mosaicml/mpt-7b-instruct,LLM,7.0,Chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,MPT 7B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
74,falcon-7b-instruct,https://huggingface.co/tiiuae/falcon-7b-instruct,LLM,7.0,Chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Falcon 7B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
75,falcon-180B,https://huggingface.co/tiiuae/falcon-180B,LLM,180.0,Advanced chat,>90,90.0,64–<128 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Large: prefer multi-GPU or server-class VRAM; enable paged attention & flash-attn if supported.,Extremely large; multi-GPU.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
76,bloom-7b1,https://huggingface.co/bigscience/bloom-7b1,LLM,7.1,Chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,BLOOM 7B1.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
77,RedPajama-INCITE-7B-Instruct,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct,LLM,7.0,Chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,RP-INCITE 7B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
78,chatglm3-6b,https://huggingface.co/THUDM/chatglm3-6b,LLM,6.0,Chat,4–5,4.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,GLM 6B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
79,rwkv-6-world,https://huggingface.co/BlinkDL/rwkv-6-world,RWKV,,RWKV experiments,4–10,4.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,GGUF (rwkv); bnb 4/8,"For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Varies by checkpoint.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
80,Phi-3-mini-4k-instruct,https://huggingface.co/microsoft/Phi-3-mini-4k-instruct,SLM,3.8,Lightweight chat,3–4,3.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Tiny model: great for edge; consider int4 + CPU fallback if VRAM tight.,Phi-3 Mini 3.8B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Phi-style chat (see card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
81,Yi-1.5-72B-Chat,https://huggingface.co/01-ai/Yi-1.5-72B-Chat,LLM,72.0,Advanced chat,38–45,38.0,32–<64 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Large: prefer multi-GPU or server-class VRAM; enable paged attention & flash-attn if supported.,Large; multi-GPU ideal.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
82,Yi-1.5-6B-Chat,https://huggingface.co/01-ai/Yi-1.5-6B-Chat,LLM,6.0,Chat,4–5,4.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Yi 6B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
83,Baichuan2-13B-Chat,https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat,LLM,13.0,Chat,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Baichuan 13B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
84,Baichuan2-7B-Chat,https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat,LLM,7.0,Chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Baichuan 7B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
85,internlm2-chat-7b,https://huggingface.co/internlm/internlm2-chat-7b,LLM,7.0,Chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,InternLM2 7B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
86,internlm2-chat-20b,https://huggingface.co/internlm/internlm2-chat-20b,LLM,20.0,Chat,12–14,12.0,12–<16 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,InternLM2 20B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
87,XVERSE-13B,https://huggingface.co/xverse/XVERSE-13B,LLM,13.0,Chat,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,XVERSE 13B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
88,xglm-7.5B,https://huggingface.co/facebook/xglm-7.5B,LLM,7.5,Multilingual,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,XGLM 7.5B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
89,pythia-12b,https://huggingface.co/EleutherAI/pythia-12b,LLM,12.0,Pretraining / experiments,8–10,8.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,EleutherAI Pythia.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
90,mpt-30b-instruct,https://huggingface.co/mosaicml/mpt-30b-instruct,LLM,30.0,Advanced chat,16–20,16.0,16–<18 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Large: prefer multi-GPU or server-class VRAM; enable paged attention & flash-attn if supported.,Large 30B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
91,mpt-30b-chat,https://huggingface.co/mosaicml/mpt-30b-chat,LLM,30.0,Chat,16–20,16.0,16–<18 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Large: prefer multi-GPU or server-class VRAM; enable paged attention & flash-attn if supported.,Large 30B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
92,Nous-Hermes-2-Mixtral-8x7B-SFT,https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT,LLM (MoE),45.0,Assistant chat,24–28,24.0,24–<32 GB,Yes (Transformer decoder; vLLM/HF),Recommended (tensor parallel via HF Accelerate / vLLM / Deepspeed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",MoE: active params << total; VRAM driven by active experts; good throughput on GPU.,Mixtral SFT.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
93,WizardCoder-Python-34B-V1.0,https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0,Code LLM,34.0,Python coding,18–22,18.0,18–<24 GB,Yes (Transformer decoder; vLLM/HF),Optional (model parallel if needed),"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",Large: prefer multi-GPU or server-class VRAM; enable paged attention & flash-attn if supported.,Large 34B code.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
94,Replit-Code-v2-13B,https://huggingface.co/replit/Replit-Code-v2-13B,Code LLM,13.0,Coding,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Replit 13B code.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Code-style prompts; tool/codemodel format,Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
95,WizardMath-13B-V1.1,https://huggingface.co/WizardLM/WizardMath-13B-V1.1,Math LLM,13.0,Math reasoning,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Math-tuned 13B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
96,dolphin-mistral (Ollama),https://ollama.com/library/dolphin-mistral,LLM (fine-tune),7.0,Helpful chat,5–6,5.0,4–<8 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Mistral 7B finetune.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Mistral-Instruct (alpaca-like),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
97,Orca-2-13b,https://huggingface.co/microsoft/Orca-2-13b,LLM,13.0,Reasoning/chat,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Microsoft Orca-2 13B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
98,phi-2,https://huggingface.co/microsoft/phi-2,SLM,2.7,Lightweight chat,2–3,2.0,2–<4 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Phi-2 2.7B.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Phi-style chat (see card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
99,Llama-2-13b-chat-hf,https://huggingface.co/meta-llama/Llama-2-13b-chat-hf,LLM,13.0,Chat,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Llama 2 13B chat.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
100,vicuna-13b-v1.5,https://huggingface.co/lmsys/vicuna-13b-v1.5,LLM (fine-tune),13.0,Assistant chat,9–11,9.0,8–<12 GB,Yes (Transformer decoder; vLLM/HF),Not needed for single-GPU,"GGUF (llama.cpp), AWQ, GPTQ, bitsandbytes 4/8 where available","For repeatability: temperature 0.0–0.2, top_p=1.0, fix random seed; use greedy/beam for most deterministic outputs.",General: use 4-bit quant + paged KV cache; keep batch small on consumer GPUs.,Vicuna 13B on LLaMA.,Varies by checkpoint (see URL),User-config / sampling dependent,Hardware + quant + batch dependent; measure on target GPU,Text-only,N/A,Generic (refer model card),Refer model card (SentencePiece/BPE/Tokenizer JSON),HF Transformers; vLLM where supported; llama.cpp/GGUF if available; Ollama for popular checkpoints,GGUF; AWQ/GPTQ/bitsandbytes 4/8 when available,1–4 (consumer GPUs); increase on larger VRAM,Use FlashAttention/paged KV cache if supported; reduce max_new_tokens for lower latency,Increase batch on bigger VRAM; prefer tensor-parallel for >24 GB models,Ensure right chat template; avoid excessive system prompts,"Check license + weights access (e.g., Llama family gated)","For determinism: temperature 0.0–0.2, top_p=1.0; set seeds; disable sampling for exact reproducibility"
