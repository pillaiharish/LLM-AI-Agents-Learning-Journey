<table><tr><td></td><td>\(N\)density</td><td>\(d_{H}\)</td><td>\(h\)</td><td>\(d_{b}\)</td><td>\(d_{c}\)</td><td>\(P_{\text {exp}}\) \(e_{L}\)</td><td>train<br>(dev)</td><td>PPLL</td><td>BLEU</td><td>params<br>&lt;10<sup>-6</sup></td></tr><tr><td rowspan="3">base</td><td>6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>0.1</td><td>0.1</td><td>100K</td><td>4.92</td><td>25.8</td><td>65</td></tr><tr><td>1</td><td>5</td><td>32</td><td>12</td><td></td><td></td><td></td><td>5.29</td><td>24.9</td><td></td><td></td></tr><tr><td>(A)</td><td></td><td></td><td>4</td><td>128</td><td>128</td><td></td><td></td><td>5.00</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td>16</td><td>32</td><td>32</td><td></td><td></td><td>4.91</td><td>25.8</td><td></td><td></td></tr><tr><td>(B)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.16</td><td>25.1</td><td></td><td>58</td></tr><tr><td>(C)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.11</td><td>25.4</td><td>60</td><td></td></tr><tr><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>23.7</td><td>30</td><td></td></tr><tr><td></td><td>8</td><td>256</td><td></td><td>32</td><td></td><td></td><td></td><td>4.88</td><td>25.3</td><td>50</td><td></td></tr><tr><td></td><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td>5.75</td><td>24.8</td><td>28</td><td></td></tr><tr><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>18</td><td></td></tr><tr><td>(D)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.77</td><td>25.2</td><td>20</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>5.77</td><td>24.6</td><td>90</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.95</td><td>25.5</td><td></td><td></td></tr><tr><td>(E)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.67</td><td>25.1</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.47</td><td>25.7</td><td></td><td></td></tr><tr><td>(Exp.)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.27</td><td>25.7</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>In Table[Brows (B), we observe that reducing the attention key size \(d_{k}\) hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function that can reduce trust may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings \(\mathbf {\Xi }_{i}^{l}\), and observe nearly identical results to the base model.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>


# 7 Conclusion


In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.


For translation tasks,the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. As in both WMT 2014 plug-ins-Genius and WMT 2014 English-to-French translation tasks,we achieve a new state of the art. In the former task one best model outperforms even all previously reported ensembles.


We are well aware that the future of automatic head models and plan to apply them to other tasks: We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local,restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.


The code we used to train and evaluate our models is available at https://github.com/ [tensorflow/tensorflow]
