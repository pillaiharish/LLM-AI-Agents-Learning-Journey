Where the projections are parameter matrices \(W_{1}^{Q} \in \mathbb{R}^{(d_{\mathrm{out}})\times d_{\mathrm{L}}} , W_{1}^{K} \in \mathbb{R}^{(k_{\mathrm{out}}\times d_{\mathrm{L}})}, W_{1}^{V} \in \mathbb{R}^{(k_{\mathrm{out}}\times d_{\mathrm{V}})}\) and \(W^{O} \in \mathbb{R}^{(k_{\mathrm{o}}\times k_{\mathrm{out}})}.\)  


In this work we employ \(h = 8\) parallel attention layers, or heads. For each of these we use \(d_{\mathrm{L}} = d_{\mathrm{v}}, d_{\mathrm{output}} / h = 64\) . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.  


#### 3.2.3 Applications of Attention in our Model  


The Transformer uses multi-head attention in three different ways:  


a. In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as BERT. b. The encoder contains self-attention layers. It's an self-attention layer all of the keys, values and queries come from the same place; in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.   


Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward propagation through the decoder if it attends to a position at time step \(t > t_0\) by using attention units inside of scaled dot-product attention by masking out (setting \(\mathbf{x} = - \mathbf{x}\) ) until values in the input of the decoder which correspond to illegal connections. See Figure 2.  


### 3.3 Position-wise Feed-Forward Networks  


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with an ReLU activation function:  

\[\operatorname {FFN}(x) = \max (0,x\mathbf{W}_1 + b_1)\mathbf{W}_2 + b_2\]  


While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is \(d_{\mathrm{input}} = 512\) , and the inner-layer has dimensionality \(d_{ff} = 2048\) .  


#### 3.4 Embeddings and Softmax  


Similarly to other sequence transformation modules, we use learned embeddings to convert the input token and output location to vectors of dimension \(d_{\mathrm{token}}\) . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to Eq. (2). In the embedding layers, we multiply those weights by \(\sqrt{d_{\mathrm{token}}}\) .  


## 3.5 Positional Encoding  


Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the
