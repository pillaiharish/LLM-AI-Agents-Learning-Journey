- Avoid using images in your report.

Table I: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types \(i\) is the sequence length, \(\beta\) is the representation dimension, \(k\) is the kernel size of convolution and \(r\) the stride of the convolutional kernel in the resized self-attention block.

| Layer Type | Complexity Per Layer | Sequential | Maximum Path Length |
|------------|----------------------|-----------|--------------------|
| Self-Attention | \(O(n^2 \cdot d)\) | \(O(1)\)     | \(O(1)\)              |
| Recurrent    | \(O(n \cdot d + c^2)\)   | \(O(n)\)     | \(O(n)\)             |
| Convolution  | \(O(k \cdot n + d^2)\)   | \(O(1)\)     | \(O(log_3(n))\)       |
| Self-Attention (restricted) | \(O(r \cdot n \cdot d)\) | \(O(1)\)     | \(O(n/r)\)          |

bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \(d_{model}\) as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed \((\mathbf{f})\)  

In this work, we use sine and cosine functions of different frequencies:

\[PE_{(pos,n)} = sin(pos/1000)^{2/(d_{model})} \\
EP_{(pos,n+1)} = cos(pos/1000)^{2/(d_{model})}\]

where pos is the position and n is the dimension. That is, each dimension of the positional encoding corresponds to a neural state. The non-linear transformation function from the x=10000, z=50, thus the chosen function because we hypothesized it would allow the model to easily learn to attend by relative positions, since far away (far off-center), \(E.P_{(pos,n)}\) can be represented as a linear function of \(PE_{(pos)}\). 

We also experimented with using learned positional embeddings \(\mathbf{ID}_{n}^{j}\) instead, and found that the two versions produced nearly identical results (see Table [Show Example]). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.

4 Why Self-Attention

In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping raw variable-length sequences of symbol representations \((x_{1},...,x_{n})\) to another sequence of equal length \((z_{1},...,z_{n})\), which \(x_{i}\in Z^{l}\), where A is a hidden layer in a typical sequence transformer encoder or decoder. Motivating our use of self-attention was consider three desiderata.  
One is the total computational complexity per layer. Another is the amount of computation that can be parallelised, as measured by the minimum number of sequential operations required. This third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transformers tasks. Once Key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals how to traverse in the network. The shorter these paths between any combination of positions in the input and output sequence, the easier it is to learn long-range dependencies \((\mathbf{I})\). Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted at Section 3, attention mechanisms require an additional parameter space to represent the context vector.
