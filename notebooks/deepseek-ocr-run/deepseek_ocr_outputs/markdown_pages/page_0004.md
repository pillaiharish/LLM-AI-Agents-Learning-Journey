<center>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.</center>  


query with all keys, divide each by \(\sqrt{d_{k}}\) , and apply a softmax function to obtain the weights on the values.  


In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix \(Q\) . The keys and values are also packed either into matrices \(K\) and \(V\) . We compute the matrix of outputs as:  


\[\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\biggl (\frac{Q K^{\top}}{\sqrt{d}}\biggr)V \quad (1)\]  


The two most commonly used attention functions are additive attention \(\boxed{\mathbf{A}}\) , and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \(\frac{1}{\sqrt{d_k}}\) . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  


While for small values of \(d_{k}\) the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of \(d_{k}\) . Q. We suspect that for large values of \(d_{k}\) , the dot products grow large in magnitude, pushing the softmax function into regions where it extremely small gradients. To counteract this effect, we scale the dot products by \(\frac{1}{\sqrt{d_{k}}}\) .  


#### 3.2.2 Multi-Head Attention  


Instead of performing a single attention function with \(d_{\mathrm{head}}\) - dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values \(q\) times with different, learned linear projections to \(d_{h}, d_{k}\) and \(d_{v}\) dimensions, respectively. On each of these projected versions of queries, keys and values, we then perform the attention function in parallel, yielding \(d_{k}\) - dimensional vectors from which we concatenated and once again projected, resulting in the final values, as depicted in Figure 3.  


Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this
