Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newsstories2014 testset at a fraction of the training cost.

<table><tr><td rowspan="3">Model</td><td colspan="5">BLEU1</td></tr><tr><td>EN-DE</td><td>EN-BF</td><td>FR-DE</td><td>EN-FR</td><td></td></tr><tr><td>25.75</td><td>39.2</td><td></td><td></td><td></td></tr><tr><td>ByteSeq [15]</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Deep-Lm + PoLMix [28]</td><td>24.6</td><td>39.92</td><td>2.3×10¹⁰</td><td>1.4×10²⁵</td><td></td></tr><tr><td>GNNIT + RL [21]</td><td>25.16</td><td>46.46</td><td>9.0×10³</td><td>1.5×10⁴⁶</td><td></td></tr><tr><td>Cos-SSIS [26]</td><td>25.16</td><td>40.45</td><td>2.0×10¹⁷</td><td>1.2×10²⁸</td><td></td></tr><tr><td>MoE [20]</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Deep-Lm + PoLMix Ensemble [33]</td><td>26.30</td><td>41.14</td><td>1.5×10²⁹</td><td>1.1×10³⁰</td><td></td></tr><tr><td>GNNIT + RL Ensemble [31]</td><td>26.30</td><td>41.19</td><td>7.7×10²⁹</td><td>1.1×10³¹</td><td></td></tr><tr><td>CosSSI Ensemble [36]</td><td>26.35</td><td>42.18</td><td>7.3×10³¹</td><td>3.3×10⁴⁴</td><td></td></tr><tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td></td><td>3.3×10⁴⁵</td><td></td></tr><tr><td>Transformer (big)</td><td>**28.4**</td><td>41.0</td><td>2.3×10⁵⁹</td><td></td><td></td></tr></table>


Label Smoothing During training, we employed label smoothing of value \(s_{L} = 0.1\) [50]. This hurts perplexity, as the model learns to be more sure, but improves accuracy and BLEU score.


# 6 Results


## 6.1 Machine Translation


On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)) in Table 3 outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is identical to that used in Table 2 during Training task 3.5 days so FID@100 GRFs. Even one less model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the compared models.


On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previous published single model results, aloee its team 1/4 the training cost of the previous state-of-the-art model. The significant (big) result gained four English-to-French sub-datasets:


For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written in 10-minute intervals. For the big models, we averaged both an IOU checkpoint. We used beam search with a beam size of 4 and length penalty \(\alpha = 0.05\) . These hyperparameters were chosen after experimentation on the development set. We see the maximum output length during inference to input length $=50$ ,but terminate early when possible [32].


Table 3 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point outputs needed to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision-floating-point capacity of each GPU.1


## 6.2 Model Variations


To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newsstory2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.


In Table 3(one), A(x) we vary the number of attention heads and the attention layer's total dimension, keeping the amount of computation constant, as described in Section 3.2[52]. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.
