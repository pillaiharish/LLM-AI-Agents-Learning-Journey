{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b3ea99",
   "metadata": {},
   "source": [
    "# ERNIE-4.5-VL-28B-A3B-Thinking-AWQ-4bit — vLLM Notebook\n",
    "\n",
    "This notebook serves `cyankiwi/ERNIE-4.5-VL-28B-A3B-Thinking-AWQ-4bit` via vLLM (OpenAI-compatible API).\n",
    "\n",
    "**System Requirements:**\n",
    "- GPU: A100 with ~40GB VRAM\n",
    "- CPU RAM: 80GB+\n",
    "- Runtime: Colab GPU, Linux\n",
    "\n",
    "**What’s in this notebook:**\n",
    "- vLLM installation (nightly wheels)\n",
    "- Start server (background) and readiness check\n",
    "- Text and image+text chat completion examples\n",
    "- Clean shutdown\n",
    "\n",
    "Notes: This vLLM path avoids the Transformers compressed-tensors route that caused `.weight` attribute errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb28990",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "- Ensure a GPU runtime is enabled\n",
    "- Install vLLM (nightly) which will install a compatible PyTorch build\n",
    "- Verify GPU with `nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aed7e0",
   "metadata": {},
   "source": [
    "## 2. Memory Monitoring Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102360aa",
   "metadata": {},
   "source": [
    "## Notes and Troubleshooting\n",
    "\n",
    "### Memory Optimization Tips:\n",
    "1. **AWQ 4-bit quantization** reduces memory footprint significantly (~4x less than FP16)\n",
    "2. **device_map=\"auto\"** automatically distributes model across GPU and CPU when needed\n",
    "3. The model should fit comfortably in 39GB VRAM with headroom for activations\n",
    "4. Only 3B parameters are activated per inference (sparse MoE architecture)\n",
    "\n",
    "### Model-Specific Features:\n",
    "- **Vision-Language**: Supports both text-only and image+text inputs\n",
    "- **Thinking Mode**: Multi-step reasoning for complex tasks\n",
    "- **Tool Calling**: Can use external tools (requires vLLM for full support)\n",
    "- **Video Understanding**: Temporal awareness and event localization\n",
    "- **Visual Grounding**: Precise object localization and grounding\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**Missing autoawq Package:**\n",
    "- Install with: `!pip install autoawq>=0.2.0`\n",
    "- This is required for loading AWQ quantized models\n",
    "\n",
    "**Out of Memory:**\n",
    "- Reduce `max_new_tokens` to 256 or 512\n",
    "- Lower batch size (use single examples)\n",
    "- Enable more aggressive CPU offloading: `max_memory={0: \"36GB\", \"cpu\": \"80GB\"}`\n",
    "\n",
    "**Slow Inference:**\n",
    "- AWQ quantization provides excellent speed vs quality tradeoff\n",
    "- Expect 20-40 tokens/sec on A100 for this model size\n",
    "- Use `do_sample=False` for faster greedy decoding\n",
    "- For production use, consider vLLM (see model card for setup)\n",
    "\n",
    "**Model Loading Errors:**\n",
    "- Ensure `trust_remote_code=True` is set (the model uses custom modeling code)\n",
    "- Check if model requires specific transformers version (>=4.37.0)\n",
    "- Verify autoawq is properly installed\n",
    "- Make sure to add image preprocessor with `model.add_image_preprocess(processor)`\n",
    "\n",
    "**Image Processing Errors:**\n",
    "- Always use the processor to prepare inputs (not just tokenizer)\n",
    "- Follow the message format shown in the helper function\n",
    "- Images should be PIL Image objects in RGB mode\n",
    "\n",
    "### Performance Expectations:\n",
    "- **Model Size:** ~6-8GB in VRAM (AWQ 4-bit INT4)\n",
    "- **Inference Speed:** 20-40 tokens/second on A100\n",
    "- **First Token Latency:** 1-2 seconds\n",
    "- **Memory Usage:** 8-12GB VRAM during inference with typical prompts\n",
    "- **Activated Parameters:** Only 3B per forward pass (sparse MoE)\n",
    "\n",
    "### For Production Deployment:\n",
    "This notebook uses `transformers` for simplicity. For production, the model card recommends using **vLLM** for better performance:\n",
    "\n",
    "```bash\n",
    "pip install uv\n",
    "uv pip install -U vllm --pre \\\n",
    "  --extra-index-url https://wheels.vllm.ai/nightly \\\n",
    "  --extra-index-url https://download.pytorch.org/whl/cu129 \\\n",
    "  --index-strategy unsafe-best-match\n",
    "\n",
    "vllm serve cyankiwi/ERNIE-4.5-VL-28B-A3B-Thinking-AWQ-4bit --trust-remote-code \\\n",
    "  --reasoning-parser ernie45 \\\n",
    "  --tool-call-parser ernie45 \\\n",
    "  --enable-auto-tool-choice\n",
    "```\n",
    "\n",
    "### Additional Resources:\n",
    "- Model Card: https://huggingface.co/cyankiwi/ERNIE-4.5-VL-28B-A3B-Thinking-AWQ-4bit\n",
    "- Base Model: https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking\n",
    "- Demo Space: https://huggingface.co/spaces/baidu/ERNIE-4.5-VL-28B-A3B-Thinking\n",
    "- AutoAWQ: https://github.com/casper-hansen/AutoAWQ\n",
    "- vLLM: https://github.com/vllm-project/vllm\n",
    "- Transformers Docs: https://huggingface.co/docs/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493969f",
   "metadata": {},
   "source": [
    "## 8. vLLM Inference (Recommended)\n",
    "\n",
    "This section serves the same model via vLLM using the OpenAI-compatible API. It avoids the Transformers compressed-tensors path and is the configuration recommended by the model card.\n",
    "\n",
    "Steps:\n",
    "1. Install vLLM (will install a matching CUDA+PyTorch build)\n",
    "2. Start the server in the background and wait for it to load\n",
    "3. Send a test chat completion request\n",
    "4. (Optional) Stop the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac315c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM (nightly wheels) — no custom pip flags\n",
    "!pip install -q -U --pre vllm \\\n",
    "  --extra-index-url https://wheels.vllm.ai/nightly\n",
    "\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "# Sanity-check import\n",
    "try:\n",
    "    import vllm\n",
    "    import torch\n",
    "    print(f\"✓ vLLM version: {getattr(vllm, '__version__', 'unknown')}\")\n",
    "    print(f\"✓ Torch version: {torch.__version__}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"⚠ Error importing vllm/torch after install:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM server in background and wait until ready\n",
    "import subprocess, time, requests, os, sys\n",
    "\n",
    "MODEL_ID = globals().get(\"MODEL_ID\", \"cyankiwi/ERNIE-4.5-VL-28B-A3B-Thinking-AWQ-4bit\")\n",
    "PORT = int(os.environ.get(\"VLLM_PORT\", \"8000\"))\n",
    "HOST = os.environ.get(\"VLLM_HOST\", \"127.0.0.1\")\n",
    "\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--model\", MODEL_ID,\n",
    "    \"--host\", HOST,\n",
    "    \"--port\", str(PORT),\n",
    "    \"--trust-remote-code\",\n",
    "]\n",
    "\n",
    "# Add optional tuning flags via env vars\n",
    "gpu_util = os.environ.get(\"VLLM_GPU_UTILIZATION\")\n",
    "if gpu_util:\n",
    "    cmd += [\"--gpu-memory-utilization\", gpu_util]\n",
    "\n",
    "print(\"Launching:\", \" \".join(cmd))\n",
    "\n",
    "# Write logs to file to keep cell responsive\n",
    "log_path = \"/tmp/vllm_server.log\"\n",
    "with open(log_path, \"w\") as log_f:\n",
    "    proc = subprocess.Popen(cmd, stdout=log_f, stderr=subprocess.STDOUT)\n",
    "    \n",
    "os.environ[\"VLLM_PROC_PID\"] = str(proc.pid)\n",
    "print(f\"vLLM server starting (pid={proc.pid}). Logs: {log_path}\")\n",
    "\n",
    "# Give it a moment to start, then check if it's still alive\n",
    "time.sleep(3)\n",
    "if proc.poll() is not None:\n",
    "    print(f\"⚠ Process exited early with code {proc.returncode}. Check logs:\")\n",
    "    with open(log_path, \"r\") as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"✓ Process still running, waiting for HTTP readiness...\")\n",
    "\n",
    "# Poll server readiness\n",
    "base_url = f\"http://{HOST}:{PORT}\"\n",
    "ready = False\n",
    "for i in range(180):  # up to ~3 minutes\n",
    "    # First check if process is still alive\n",
    "    if proc.poll() is not None:\n",
    "        print(f\"⚠ Server process died (exit code {proc.returncode}). Check logs.\")\n",
    "        break\n",
    "        \n",
    "    try:\n",
    "        r = requests.get(base_url + \"/v1/models\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            ready = True\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "\n",
    "if ready:\n",
    "    print(\"✓ vLLM server ready!\")\n",
    "else:\n",
    "    print(\"⚠ vLLM server not ready. Check logs with the log viewer cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afcd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM client: text-only chat completion\n",
    "import requests, json, os\n",
    "\n",
    "PORT = int(os.environ.get(\"VLLM_PORT\", \"8000\"))\n",
    "HOST = os.environ.get(\"VLLM_HOST\", \"127.0.0.1\")\n",
    "base_url = f\"http://{HOST}:{PORT}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_ID = globals().get(\"MODEL_ID\", \"cyankiwi/ERNIE-4.5-VL-28B-A3B-Thinking-AWQ-4bit\")\n",
    "\n",
    "# Health check first\n",
    "try:\n",
    "    health_r = requests.get(base_url + \"/v1/models\", timeout=5)\n",
    "    if health_r.status_code != 200:\n",
    "        print(f\"⚠ Server health check failed: {health_r.status_code}\")\n",
    "        print(\"Server may not be ready. Run the log viewer to debug.\")\n",
    "        raise Exception(f\"Health check failed: {health_r.status_code}\")\n",
    "    else:\n",
    "        print(\"✓ Server health check passed\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"⚠ Cannot connect to vLLM server. Is it running?\")\n",
    "    print(\"Run the server start cell first, then check logs if it fails.\")\n",
    "    raise\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "    ],\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "print(\"Sending chat completion request...\")\n",
    "r = requests.post(base_url + \"/v1/chat/completions\", headers=headers, data=json.dumps(payload), timeout=180)\n",
    "print(f\"Status: {r.status_code}\")\n",
    "\n",
    "if r.status_code == 200:\n",
    "    response = r.json()\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(f\"Error: {r.status_code}\")\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63519882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) vLLM client: simple image+text example\n",
    "import requests, json, os\n",
    "\n",
    "PORT = int(os.environ.get(\"VLLM_PORT\", \"8000\"))\n",
    "HOST = os.environ.get(\"VLLM_HOST\", \"127.0.0.1\")\n",
    "base_url = f\"http://{HOST}:{PORT}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_ID = globals().get(\"MODEL_ID\", \"cyankiwi/ERNIE-4.5-VL-28B-A3B-Thinking-AWQ-4bit\")\n",
    "\n",
    "# Provide a publicly accessible image URL\n",
    "image_url = \"https://paddlenlp.bj.bcebos.com/datasets/paddlemix/demo_images/example1.jpg\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What color clothes is the girl in the picture wearing?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "r = requests.post(base_url + \"/v1/chat/completions\", headers=headers, data=json.dumps(payload), timeout=180)\n",
    "print(r.status_code)\n",
    "print(r.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop vLLM server and free resources\n",
    "import os, signal, time\n",
    "pid = int(os.environ.get(\"VLLM_PROC_PID\", \"0\"))\n",
    "if pid > 0:\n",
    "    try:\n",
    "        os.kill(pid, signal.SIGTERM)\n",
    "        time.sleep(2)\n",
    "        print(f\"✓ Terminated vLLM server (pid={pid})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not terminate server pid={pid}: {e}\")\n",
    "else:\n",
    "    print(\"No vLLM server PID found. If needed, run: !pkill -f 'vllm serve'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View last 100 lines of vLLM server logs (for debugging)\n",
    "import os\n",
    "log_path = \"/tmp/vllm_server.log\"\n",
    "if os.path.exists(log_path):\n",
    "    with open(log_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-100:]:\n",
    "            print(line.rstrip())\n",
    "else:\n",
    "    print(\"Log file not found. Start the server first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
